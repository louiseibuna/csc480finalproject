{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import layers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import json\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filters dataframe to only contain tweets from specified usernames\n",
    "def filter_by_username(depressed, username):\n",
    "    columns = ['likes', 'replies', 'retweets', 'text', 'timestamp', 'user']\n",
    "    filtered = pd.DataFrame(columns=columns)\n",
    "    for index, row in depressed.iterrows():\n",
    "        temp = {}\n",
    "        if row['user'] in usernames:\n",
    "            temp['likes'] = row['likes']\n",
    "            temp['replies'] = row['replies']\n",
    "            temp['retweets'] = row['retweets']\n",
    "            temp['text'] = row['text']\n",
    "            temp['timestamp'] = row ['timestamp']\n",
    "            temp['user'] = row ['user']\n",
    "        \n",
    "            filtered = filtered.append(temp, ignore_index=True)\n",
    "    filtered['likes'] = filtered['likes'].astype(int)\n",
    "    filtered['retweets'] = filtered['retweets'].astype(int)\n",
    "    filtered['replies'] = filtered['replies'].astype(int)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "# generates batches of training data and respective labels\n",
    "def generator(depressed, nondepressed, batch_size=128):\n",
    "    maxlen=15\n",
    "    depressed_ct = 0\n",
    "    nondepressed_ct = 0\n",
    "    while 1:\n",
    "        # depressed is 1, nondepressed 0\n",
    "        class_ = random.randint(0,1)\n",
    "        if class_ == 0:\n",
    "            rows = nondepressed[nondepressed_ct:nondepressed_ct + batch_size]\n",
    "            nondepressed_ct += batch_size\n",
    "            if(nondepressed_ct >= len(nondepressed)):\n",
    "                nondepressed_ct = nondepressed_ct - len(nondepressed)\n",
    "                rows = rows + nondepressed[:nondepressed_ct]\n",
    "            labels = np.zeros((batch_size,1))\n",
    "        else:\n",
    "            rows = depressed[depressed_ct:depressed_ct+batch_size]\n",
    "            depressed_ct += batch_size\n",
    "            if(depressed_ct >= len(depressed)):\n",
    "                depressed_ct = depressed_ct - len(depressed)\n",
    "                rows = rows + depressed[:depressed_ct]\n",
    "            labels = np.ones((batch_size,1))\n",
    "            \n",
    "        train = sequence.pad_sequences(rows,maxlen=maxlen)\n",
    "        #print(\"\\ntrain: %i label: %i class: %i\" % (len(train), len(labels), class_))\n",
    "        yield train, labels\n",
    "        \n",
    "# extracts indivifual tweets from dataframe, \n",
    "# parses them to contain only text,\n",
    "# and returns them in a list\n",
    "def create_tweets_lists(dataframe):\n",
    "    sentences = []\n",
    "    for index, row in dataframe[['text']].iterrows():\n",
    "        text = str(row['text'])\n",
    "        text = re.sub(r'http:\\S+', '', text)\n",
    "        text = re.sub(r'https\\S+', '', text)\n",
    "        text = re.sub(r'www.\\S+', '', text)\n",
    "        text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text).strip().lower()\n",
    "        if len(text) > 0:\n",
    "            sentences.append(text)\n",
    "    return sentences\n",
    "\n",
    "# takes a dataframe and portions it into \n",
    "# training, validation, and testing portions\n",
    "def create_train_val_and_test(df):\n",
    "    users = df['user'].unique()\n",
    "    train = len(users) // 2\n",
    "    test = len(users) * 3 // 4\n",
    "    np.random.shuffle(users)\n",
    "    train_users = users[:train]\n",
    "    val_users = users[train:test]\n",
    "    test_users = users[test:]\n",
    "    train = pd.DataFrame()\n",
    "    val = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "    for user, df_user in df.groupby('user'):\n",
    "        if user in train_users:\n",
    "            train = pd.concat([train, df_user], ignore_index=True)\n",
    "        elif user in val_users:\n",
    "            val = pd.concat([val, df_user], ignore_index=True)\n",
    "        else:\n",
    "            test = pd.concat([test, df_user], ignore_index=True)\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# takes a dataframe containing test data.\n",
    "# iterates through dataframe by user and \n",
    "# predicts value for every tweet. whichever\n",
    "# value has most frequency is depression \n",
    "# prediction for the user. total depressed\n",
    "# and non depressed counts are returned.\n",
    "# (depressed is 1, nondepressed is 0)\n",
    "def predict(model, df):\n",
    "    dep = 0\n",
    "    nondep = 0\n",
    "    for user, df_user in df.groupby('user'):\n",
    "        tweets = create_tweets_lists(df_user)\n",
    "        sequences = tokenizer.texts_to_sequences(tweets)\n",
    "        test = sequence.pad_sequences(sequences, maxlen=15)\n",
    "        preds = model.predict_classes(test)\n",
    "        preds = preds.T[0]\n",
    "        prediction = np.bincount(preds).argmax()\n",
    "        if prediction == 1:\n",
    "            dep += 1\n",
    "        else:\n",
    "            nondep += 1\n",
    "            \n",
    "    return dep, nondep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting and formatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "depressed = pd.read_csv(\"depressedTweets.csv\")\n",
    "non_depressed = pd.read_csv('non_depressed_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_train, dep_val, dep_test = create_train_val_and_test(depressed)\n",
    "non_train, non_val, non_test = create_train_val_and_test(non_depressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_train = create_tweets_lists(dep_train)\n",
    "dep_val = create_tweets_lists(dep_val)\n",
    "non_train = create_tweets_lists(non_train)\n",
    "non_val = create_tweets_lists(non_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = dep_train + non_train + dep_val + non_val\n",
    "b1 = len(dep_train)\n",
    "b2 = b1 + len(non_train)\n",
    "b3 = b2 + len(dep_val)\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(all_tweets)\n",
    "sequences = tokenizer.texts_to_sequences(all_tweets)\n",
    "d_train = sequences[:b1]\n",
    "n_train = sequences[b1:b2]\n",
    "d_val = sequences[b2:b3]\n",
    "n_val = sequences[b3:]\n",
    "np.random.shuffle(d_train)\n",
    "np.random.shuffle(n_train)\n",
    "np.random.shuffle(d_val)\n",
    "np.random.shuffle(n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(d_train, n_train)\n",
    "val_gen = generator(d_val, n_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = len(tokenizer.word_index)\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(embeds, 64))\n",
    "model.add(layers.LSTM(32, return_sequences=True))\n",
    "model.add(layers.LSTM(32))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "200/200 [==============================] - 9s 47ms/step - loss: 0.6011 - acc: 0.6410 - val_loss: 0.6872 - val_acc: 0.5606\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.4286 - acc: 0.7887 - val_loss: 0.7759 - val_acc: 0.5556\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.3457 - acc: 0.8402 - val_loss: 0.8575 - val_acc: 0.5714\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.3002 - acc: 0.8620 - val_loss: 1.0290 - val_acc: 0.5656\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.2723 - acc: 0.8752 - val_loss: 1.0413 - val_acc: 0.5508\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.2432 - acc: 0.8935 - val_loss: 1.1446 - val_acc: 0.5644\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.2198 - acc: 0.9035 - val_loss: 1.2320 - val_acc: 0.5619\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.1974 - acc: 0.9166 - val_loss: 1.5904 - val_acc: 0.5148\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.1730 - acc: 0.9278 - val_loss: 1.4248 - val_acc: 0.5794\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 0.1644 - acc: 0.9313 - val_loss: 1.5262 - val_acc: 0.5445\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 8s 42ms/step - loss: 0.1450 - acc: 0.9387 - val_loss: 1.5830 - val_acc: 0.5745\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.1330 - acc: 0.9433 - val_loss: 1.8323 - val_acc: 0.5683\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 8s 40ms/step - loss: 0.1260 - acc: 0.9469 - val_loss: 1.8680 - val_acc: 0.5766\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.1121 - acc: 0.9519 - val_loss: 2.2759 - val_acc: 0.5316\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 8s 41ms/step - loss: 0.1006 - acc: 0.9575 - val_loss: 2.5288 - val_acc: 0.5439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb39d7bdd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_gen,\n",
    "    steps_per_epoch=200,\n",
    "    epochs=15,\n",
    "    validation_data=val_gen,\n",
    "    validation_steps=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('95_percent.h5')\n",
    "dep_test = pd.read_csv('depressedTest.csv')\n",
    "non_test = pd.read_csv('nondepressedTest.csv')\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.95\n",
      "97.5\n"
     ]
    }
   ],
   "source": [
    "dep_size = len(dep_test.groupby('user'))\n",
    "non_size = len(non_test.groupby('user'))\n",
    "\n",
    "dep_count, nondep_count = predict(model, dep_test)\n",
    "dep_acc = dep_count / dep_size\n",
    "dep_count, nondep_count = predict(model, non_test)\n",
    "non_acc = dep_count / non_size\n",
    "\n",
    "total_size = dep_size + non_size\n",
    "dep_weight = dep_size / total_size\n",
    "non_weight = non_size / total_size\n",
    "\n",
    "weight_acc = (dep_acc * 100 * dep_weight) + (non_acc * 100 * non_weight)\n",
    "print(dep_acc)\n",
    "print(non_acc)\n",
    "print(weight_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('95_percent.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_test.to_csv('depressedTest.csv')\n",
    "non_test.to_csv('nondepressedTest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_json = tokenizer.to_json()\n",
    "with io.open('tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
